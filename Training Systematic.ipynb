{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40zPtJsx_njC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# drive mouting and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uzF3SdVm_Zy7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654679222320,
     "user_tz": -120,
     "elapsed": 1571,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "outputId": "c3dd7039-8fa4-487b-f907-074882c40326",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#mounting drive and setting path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import sys \n",
    "import os\n",
    "sys.path.append('/content/drive/MyDrive/OptML_project')\n",
    "checkpoint_folder = 'checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f7Zl-bBiFF0q",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654679234072,
     "user_tz": -120,
     "elapsed": 4566,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sharpness.MinimumSimple import effective as minimum_shaprness_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1654679234075,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     },
     "user_tz": -120
    },
    "id": "-IyZV96e_k_u",
    "outputId": "d671bebd-a459-492a-a8e4-10348192d4c9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#libraries import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, CIFAR10, FashionMNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "\n",
    "#files imports\n",
    "from adashift import AdaShift\n",
    "from adabound import AdaBound\n",
    "#from sam import SAM\n",
    "from models import *\n",
    "from main import *\n",
    "\n",
    "#setting the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "DATASET = 'FashionMNIST'"
   ],
   "metadata": {
    "id": "xe0qkPrtXnUh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654686087805,
     "user_tz": -120,
     "elapsed": 418,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_model(test_data_loader, model):\n",
    "    test_statistics = test(model, test_data_loader, device=device)\n",
    "    accuracy = test_statistics['accuracy']\n",
    "    loss = test_statistics['loss']\n",
    "    print(f\"Performance on validation data:\\naccuracy : {accuracy:.2f}% | loss = {loss:.6f}\")\n",
    "\n",
    "def get_model(architecture, dataset):\n",
    "    input_channels = 3 if dataset == 'CIFAR10' else 1\n",
    "    size = 32 if dataset == 'CIFAR10' else 28\n",
    "    if architecture == 'SimpleBatch':\n",
    "        return SimpleBatch(input_channels=input_channels, size=size)\n",
    "    if architecture == 'MiddleBatch':\n",
    "        return MiddleBatch(input_channels=input_channels, size=size)\n",
    "    if architecture == 'ComplexBatch':\n",
    "        return ComplexBatch(input_channels=input_channels, size=size)\n"
   ],
   "metadata": {
    "id": "9WUGq5DGbqVw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654686089152,
     "user_tz": -120,
     "elapsed": 5,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading dataset and preporcessing data"
   ],
   "metadata": {
    "id": "uXm7a7-AXkul",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TRAIN_BATCH_SIZE = 2**7\n",
    "VAL_BATCH_SIZE = 1000\n",
    "\n",
    "if DATASET == 'CIFAR10':\n",
    "    #loading datasets\n",
    "    train_data =  CIFAR10('./data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "    ]), )\n",
    "\n",
    "    test_data = CIFAR10('./data', train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "    ]), )\n",
    "\n",
    "    #creating dataLoaders\n",
    "    train_loader = dataloader.DataLoader(train_data, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    test_loader = dataloader.DataLoader(test_data, shuffle=False, batch_size=VAL_BATCH_SIZE)\n",
    "\n",
    "if DATASET == 'FashionMNIST':\n",
    "    #loading datasets\n",
    "    train_data =  FashionMNIST('./data', train=True, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "    ]), )\n",
    "\n",
    "    test_data = FashionMNIST('./data', train=False, download=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "    ]), )\n",
    "\n",
    "    #creating dataLoaders\n",
    "    train_loader = dataloader.DataLoader(train_data, shuffle=True, batch_size=TRAIN_BATCH_SIZE)\n",
    "    test_loader = dataloader.DataLoader(test_data, shuffle=False, batch_size=VAL_BATCH_SIZE)"
   ],
   "metadata": {
    "id": "lqX34zqAX45i",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654686090947,
     "user_tz": -120,
     "elapsed": 388,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f'Preporcessing dataset {DATASET}...')\n",
    "begin = datetime.now()\n",
    "\n",
    "x = torch.stack([v[0] for v in train_data])\n",
    "y = torch.tensor(train_data.targets)\n",
    "\n",
    "#x, y = x.to(device), y.to(device)\n",
    "x = x.cuda()\n",
    "y = y.cuda()\n",
    "\n",
    "data = namedtuple('_','x y n')(x=x, y=y,n=len(y))\n",
    "\n",
    "print(f'Time needed {datetime.now() - begin}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yigjHKr6X3Mm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654686096960,
     "user_tz": -120,
     "elapsed": 5425,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "outputId": "4fe1ed02-3791-4f60-c2e6-418170a8a707",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preporcessing dataset FashionMNIST...\n",
      "Time needed 0:00:04.697079\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "epoch = 200\n",
    "architecture = 'MiddleBatch'\n",
    "\n",
    "PATH = f'{checkpoint_folder}{DATASET}/{architecture}/epoch200/SGD_0.1.pt'\n",
    "print('Loading model...')\n",
    "\n",
    "checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "model = checkpoint['state_dict']\n",
    "model = get_model(architecture, DATASET).to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "print(f'Dataset: {DATASET} \\t Architecture: {architecture} \\t Optimizer: SGD lr = 0.1')\n",
    "print(f'Training statistics: \\t accuracy: {checkpoint[\"training_accuracy\"][-1]} \\t loss: {checkpoint[\"training_loss\"][-1]}')\n",
    "print(f'Test statistics: \\t accuracy: {checkpoint[\"validation_accuracy\"][-1]} \\t loss: {checkpoint[\"validation_loss\"][-1]}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YDDsfCVzbSzx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654686112792,
     "user_tz": -120,
     "elapsed": 395,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "outputId": "0190ef06-ea40-4284-c88b-a29515ff638c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model...\n",
      "Dataset: FashionMNIST \t Architecture: MiddleBatch \t Optimizer: SGD lr = 0.1\n",
      "Training statistics: \t accuracy: 100.0 \t loss: 6.022807643707049e-06\n",
      "Test statistics: \t accuracy: 93.79999542236328 \t loss: 0.509210342168808\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epoch = 200\n",
    "for architecture in ['SimpleBatch', 'MiddleBatch', 'ComplexBatch']:\n",
    "\n",
    "    directory = f'{checkpoint_folder}{DATASET}/{architecture}/converged'\n",
    "    files = os.listdir(directory)\n",
    "\n",
    "    print('-'*100)\n",
    "    print(architecture)\n",
    "    print('-'*100)\n",
    "\n",
    "    for filename in files:\n",
    "        if filename.endswith('_sharpness.pt') or filename.endswith('_hessian.pt'):\n",
    "            continue\n",
    "        \n",
    "\n",
    "        print(f'Current file:{filename}')\n",
    "        sharpness_filename = filename.replace('.pt', '_sharpness.pt')\n",
    "        if sharpness_filename in files:\n",
    "            print(f'For {filename.replace(\".pt\", \"\")} shrapness is already computed\\n')\n",
    "            continue\n",
    "\n",
    "        if 'adashift' in filename.lower():\n",
    "            print(f'There is sth weird with AdaShift')\n",
    "            continue\n",
    "\n",
    "        \n",
    "        path = os.path.join(directory, filename)\n",
    "        checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "        model = checkpoint['state_dict']\n",
    "        model = get_model(architecture, DATASET).to(device)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "        \n",
    "        lr = 0.1\n",
    "        num_epochs = 100000\n",
    "        computed = False\n",
    "        \n",
    "        while not computed:\n",
    "            try:\n",
    "                sharpnesses, losses = minimum_shaprness_eff(data, model, 128, lr, num_epochs=num_epochs, optimizer_file=path)\n",
    "                sharpness_path = os.path.join(directory, sharpness_filename)\n",
    "                checkpoint = {'sharpnesses':sharpnesses, 'sharpness':sharpnesses[-1], 'losses': losses}\n",
    "                torch.save(checkpoint, sharpness_path)\n",
    "\n",
    "        \n",
    "                computed = True\n",
    "                print(f'\\t{filename} done \\t sharpness: {sharpnesses[-1]}')\n",
    "            except:\n",
    "                print(f'Use smaller stepsize than {lr}')\n",
    "                computed = False\n",
    "                lr /= 2.0\n",
    "                num_epochs *= 2\n",
    "\n",
    "\n",
    "        print()\n",
    "        \n",
    "    print('-'*100)\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmzBQcM_EPUd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654697497650,
     "user_tz": -120,
     "elapsed": 11296010,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "outputId": "2e147409-c85d-4954-c78b-c0451c8b836d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "SimpleBatch\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Current file:PHB_0.1_0.8.pt\n",
      "\t Calculating Hessian\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\tFinished the diag calculation. Time needed: 0:01:10.100262. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.010402049198079463 \t minimum sharpness: 95233.87126268476 \t Time needed 0:02:01.232482\n",
      "--------------------------------------------------\n",
      "\tPHB_0.1_0.8.pt done \t sharpness: 95233.87126268476\n",
      "\n",
      "Current file:SGD_0.1.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.092269. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.012309919085253055 \t minimum sharpness: 54812.88849387634 \t Time needed 0:02:02.645844\n",
      "--------------------------------------------------\n",
      "\tSGD_0.1.pt done \t sharpness: 54812.88849387634\n",
      "\n",
      "Current file:Adam.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.083995. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.009570424038998286 \t minimum sharpness: 78887.24651684402 \t Time needed 0:02:00.312480\n",
      "--------------------------------------------------\n",
      "\tAdam.pt done \t sharpness: 78887.24651684402\n",
      "\n",
      "Current file:AdaBound.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.261820. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.012980292626710337 \t minimum sharpness: 47928.199989651745 \t Time needed 0:02:02.129057\n",
      "--------------------------------------------------\n",
      "\tAdaBound.pt done \t sharpness: 47928.199989651745\n",
      "\n",
      "Current file:AdaShift.pt\n",
      "There is sth weird with AdaShift\n",
      "Current file:Adagrad.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.076335. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.008513329903463849 \t minimum sharpness: 105184.49055863719 \t Time needed 0:02:06.874238\n",
      "--------------------------------------------------\n",
      "\tAdagrad.pt done \t sharpness: 105184.49055863719\n",
      "\n",
      "Current file:SAM_SGD_0.1.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.143249. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.01564252749980406 \t minimum sharpness: 15007.42472426795 \t Time needed 0:02:02.423902\n",
      "--------------------------------------------------\n",
      "\tSAM_SGD_0.1.pt done \t sharpness: 15007.42472426795\n",
      "\n",
      "Current file:SAM_PHB_0.1_0.8.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.128288. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.030586154321367888 \t minimum sharpness: 16364.830220753563 \t Time needed 0:02:01.998490\n",
      "--------------------------------------------------\n",
      "\tSAM_PHB_0.1_0.8.pt done \t sharpness: 16364.830220753563\n",
      "\n",
      "Current file:SAM_Adam.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.047782. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.01866801138711641 \t minimum sharpness: 17949.02113752731 \t Time needed 0:02:01.350093\n",
      "--------------------------------------------------\n",
      "\tSAM_Adam.pt done \t sharpness: 17949.02113752731\n",
      "\n",
      "Current file:SAM_AdaBound.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.076039. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.01594465417101735 \t minimum sharpness: 16754.842909326126 \t Time needed 0:02:01.752215\n",
      "--------------------------------------------------\n",
      "\tSAM_AdaBound.pt done \t sharpness: 16754.842909326126\n",
      "\n",
      "Current file:SAM_Adagrad.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:01:09.148546. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.011560770965488604 \t minimum sharpness: 23760.833244701913 \t Time needed 0:02:02.951969\n",
      "--------------------------------------------------\n",
      "\tSAM_Adagrad.pt done \t sharpness: 23760.833244701913\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MiddleBatch\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Current file:PHB_0.1_0.8.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.987539. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.011164216249936984 \t minimum sharpness: 172643.39412726098 \t Time needed 0:02:42.024575\n",
      "--------------------------------------------------\n",
      "\tPHB_0.1_0.8.pt done \t sharpness: 172643.39412726098\n",
      "\n",
      "Current file:SGD_0.1.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.889365. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.012247718213618742 \t minimum sharpness: 82439.83322598794 \t Time needed 0:02:42.627984\n",
      "--------------------------------------------------\n",
      "\tSGD_0.1.pt done \t sharpness: 82439.83322598794\n",
      "\n",
      "Current file:SAM_Adam.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.855068. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.021410469504320743 \t minimum sharpness: 16435.81210481438 \t Time needed 0:02:43.054758\n",
      "--------------------------------------------------\n",
      "\tSAM_Adam.pt done \t sharpness: 16435.81210481438\n",
      "\n",
      "Current file:SAM_AdaBound.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.879399. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.015242357026653533 \t minimum sharpness: 33084.35223346137 \t Time needed 0:02:43.398952\n",
      "--------------------------------------------------\n",
      "\tSAM_AdaBound.pt done \t sharpness: 33084.35223346137\n",
      "\n",
      "Current file:SAM_AdaShift.pt.pt\n",
      "There is sth weird with AdaShift\n",
      "Current file:SAM_Adagrad.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.916439. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.012831619611363563 \t minimum sharpness: 33672.10403318422 \t Time needed 0:02:40.970997\n",
      "--------------------------------------------------\n",
      "\tSAM_Adagrad.pt done \t sharpness: 33672.10403318422\n",
      "\n",
      "Current file:Adam.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.867707. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.009527965441524327 \t minimum sharpness: 77263.11392191272 \t Time needed 0:02:40.588317\n",
      "--------------------------------------------------\n",
      "\tAdam.pt done \t sharpness: 77263.11392191272\n",
      "\n",
      "Current file:Adagrad.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.801414. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.009887580919966712 \t minimum sharpness: 371136.6014366938 \t Time needed 0:02:40.407840\n",
      "--------------------------------------------------\n",
      "\tAdagrad.pt done \t sharpness: 371136.6014366938\n",
      "\n",
      "Current file:AdaBound.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.741267. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.013419302227324221 \t minimum sharpness: 138891.12946229443 \t Time needed 0:02:40.584008\n",
      "--------------------------------------------------\n",
      "\tAdaBound.pt done \t sharpness: 138891.12946229443\n",
      "\n",
      "Current file:AdaShift.pt\n",
      "There is sth weird with AdaShift\n",
      "Current file:SAM_SGD_0.1.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:03:39.795663. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.014169806512074341 \t minimum sharpness: 15102.691212887583 \t Time needed 0:02:40.702170\n",
      "--------------------------------------------------\n",
      "\tSAM_SGD_0.1.pt done \t sharpness: 15102.691212887583\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ComplexBatch\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Current file:PHB_0.1_0.8.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:52.171217. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.014004781146589576 \t minimum sharpness: 181372.58941248208 \t Time needed 0:03:43.402543\n",
      "--------------------------------------------------\n",
      "\tPHB_0.1_0.8.pt done \t sharpness: 181372.58941248208\n",
      "\n",
      "Current file:SGD_0.1.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:52.195975. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.014940427207691558 \t minimum sharpness: 107841.69793696102 \t Time needed 0:03:44.280586\n",
      "--------------------------------------------------\n",
      "\tSGD_0.1.pt done \t sharpness: 107841.69793696102\n",
      "\n",
      "Current file:SAM_SGD_0.1.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:52.135623. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.016675817758896195 \t minimum sharpness: 16376.533695365291 \t Time needed 0:03:46.882377\n",
      "--------------------------------------------------\n",
      "\tSAM_SGD_0.1.pt done \t sharpness: 16376.533695365291\n",
      "\n",
      "Current file:Adam.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:51.190728. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.01156430731878359 \t minimum sharpness: 63556.643701032364 \t Time needed 0:03:59.318576\n",
      "--------------------------------------------------\n",
      "\tAdam.pt done \t sharpness: 63556.643701032364\n",
      "\n",
      "Current file:Adagrad.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:51.322040. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.012344913751520014 \t minimum sharpness: 723554.8521611799 \t Time needed 0:04:04.668008\n",
      "--------------------------------------------------\n",
      "\tAdagrad.pt done \t sharpness: 723554.8521611799\n",
      "\n",
      "Current file:AdaBound.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:51.166501. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.016503330564896803 \t minimum sharpness: 126051.76299096677 \t Time needed 0:04:09.927008\n",
      "--------------------------------------------------\n",
      "\tAdaBound.pt done \t sharpness: 126051.76299096677\n",
      "\n",
      "Current file:SAM_PHB_0.1_0.8.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:51.136367. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.029722900425715196 \t minimum sharpness: 160780.48608817125 \t Time needed 0:04:07.979879\n",
      "--------------------------------------------------\n",
      "\tSAM_PHB_0.1_0.8.pt done \t sharpness: 160780.48608817125\n",
      "\n",
      "Current file:SAM_Adam.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:51.112416. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.03589710719189971 \t minimum sharpness: 13181.241019637433 \t Time needed 0:04:06.397977\n",
      "--------------------------------------------------\n",
      "\tSAM_Adam.pt done \t sharpness: 13181.241019637433\n",
      "\n",
      "Current file:SAM_AdaBound.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:51.218667. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.01903813944361323 \t minimum sharpness: 29327.38386445649 \t Time needed 0:04:05.454338\n",
      "--------------------------------------------------\n",
      "\tSAM_AdaBound.pt done \t sharpness: 29327.38386445649\n",
      "\n",
      "Current file:SAM_Adagrad.pt\n",
      "\t Calculating Hessian\n",
      "\tFinished the diag calculation. Time needed: 0:05:51.298669. Computing sharpness...\n",
      " \t\t epoch:100000\t processed 100.0%\t loss:0.016723821362668832 \t minimum sharpness: 33816.79149857818 \t Time needed 0:04:07.303345\n",
      "--------------------------------------------------\n",
      "\tSAM_Adagrad.pt done \t sharpness: 33816.79149857818\n",
      "\n",
      "Current file:AdaShift.pt\n",
      "There is sth weird with AdaShift\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "! ls drive/MyDrive/OptML_project/checkpoints/CIFAR10/SimpleBatch/epoch200/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LlG6DvDM1ZX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654629499675,
     "user_tz": -120,
     "elapsed": 502,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "outputId": "0fed9a38-6bfc-44d9-f3c7-5a7416054704",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AdaBound.pt  AdaShift.pt      SAM_Adagrad.pt\t\tSAM_SGD_0.1.pt\n",
      "Adagrad.pt   PHB_0.1_0.8.pt   SAM_Adagrad_sharpness.pt\tSGD_0.1.pt\n",
      "Adam.pt      SAM_AdaBound.pt  SAM_Adam.pt\t\tSGD_0.1_sharpness.pt\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#42853.32931748071\n",
    "\n",
    "checkpoint = torch.load('drive/MyDrive/OptML_project/checkpoints/CIFAR10/SimpleBatch/epoch200/SAM_Adagrad_sharpness.pt')\n",
    "checkpoint['sharpness']"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0boHGo3hLSym",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1654629639591,
     "user_tz": -120,
     "elapsed": 529,
     "user": {
      "displayName": "Jana Vuckovic",
      "userId": "02068696855166371127"
     }
    },
    "outputId": "dc7f79e5-4c4c-4e47-fff5-d80114160579",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15788.708123962846"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(sharpnesses_SAM, label='SAM')\n",
    "plt.plot(sharpnesses_SGD, label='SGD')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "q2PlSz2-V2ET",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "losses_SGD = deepcopy(sharpnesses)\n",
    "sharpnesses_SGD = deepcopy(losses)"
   ],
   "metadata": {
    "id": "3RPDzrKqO3vU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "m-TAj8KcR9Rg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "l_tOmgYD5ZGc",
    "92G9bWYH5dE5",
    "F1R37ap257HJ",
    "u4YXxpAN5_Dt",
    "zKalkpLh6BuG",
    "icXdLLDY6EnN",
    "RWJZMe0K62fb",
    "JhU88RPP62fv",
    "snhItp4H62fw",
    "LDAcLm2N62fx",
    "zXOpQd2R62fx",
    "nlUcLuVF62fy",
    "Zq-tvu3562fy",
    "0ShgU-Vm6-AX"
   ],
   "name": "Sharpness.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}